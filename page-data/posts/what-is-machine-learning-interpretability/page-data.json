{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/what-is-machine-learning-interpretability","result":{"data":{"markdownRemark":{"id":"61caf124-459e-53f6-9e39-90dfe1e21ce4","html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 345px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/blog/static/606f3769e6b3944838d4279e4001b129/e4d6b/blackbox.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.5%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+UlEQVQoz6WRz+pEUBTHb1lKdqImCx7ByiMoKVkSUX62nsILyI7HsLLyKBayQclCGOZ3css0mtnMfBb3nnPv+fO956LHD6B932HDK7Bt2/0d+HY/eCaf1rqusIZhSFEUz/Mcx90OaJrWNO1MfuncNM0wDF3XTdMEvm3bJEmyLCsIAsMwoihCFVmW53lelqWqqrqu+75v2xbkoCzLkiQpyxJKjOMInXVdd13X931VVT3PM01TURSoDglFUaRpGsdxnudQDuFHnrINw0AIEQSBDrAhSRKEYdln/HNgAD4CIZZlBUHwdwCG4zhRFF1GdR3YN1918bcPvE3+B/wJs5HaT05kAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/blog/static/606f3769e6b3944838d4279e4001b129/8ac56/blackbox.webp 240w,\n/blog/static/606f3769e6b3944838d4279e4001b129/50aa2/blackbox.webp 345w\"\n              sizes=\"(max-width: 345px) 100vw, 345px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/blog/static/606f3769e6b3944838d4279e4001b129/8ff5a/blackbox.png 240w,\n/blog/static/606f3769e6b3944838d4279e4001b129/e4d6b/blackbox.png 345w\"\n            sizes=\"(max-width: 345px) 100vw, 345px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/blog/static/606f3769e6b3944838d4279e4001b129/e4d6b/blackbox.png\"\n            alt=\"Photo of AI models illustrated as Blackbox\"\n            title=\"Photo of AI models illustrated as Blackbox\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<figure>\n\t<blockquote>\n\t\t<p>Interpretability is the degree to which a human can understand the cause of a decision.</p>\n\t\t<footer>\n\t\t\t<cite>— Tim Miller</cite>\n\t\t</footer>\n\t</blockquote>\n</figure>\n<h2 id=\"scenarios\" style=\"position:relative;\"><a href=\"#scenarios\" aria-label=\"scenarios permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scenarios</h2>\n<ol>\n<li>Why does our trained model approve or reject certain credit applications.</li>\n<li>Why does our trained model classify someone has Alzheimer’s disease from a brain CT scan.</li>\n</ol>\n<h2 id=\"local-vs-global-interpretability\" style=\"position:relative;\"><a href=\"#local-vs-global-interpretability\" aria-label=\"local vs global interpretability permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Local vs Global Interpretability</h2>\n<ol>\n<li><strong>Local</strong>: The model explains an individual prediction.</li>\n<li><strong>Global</strong>: The model explains entire model behaviour.</li>\n</ol>\n<h2 id=\"scope-of-interpretability\" style=\"position:relative;\"><a href=\"#scope-of-interpretability\" aria-label=\"scope of interpretability permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Scope of Interpretability</h2>\n<ol>\n<li><strong>Algorithm Transparency</strong>: How much do we understand about the algorithm? <br/>\n<em>We know that Convolutional Neural Network learns simple features such as detecting edge and line on the lower layers and more complex features on the deeper layers. That’s the overall knowledge we have on the general CNN without consider whether it is trained.</em></li>\n<li><strong>Global Interpretability (Overall)</strong>: How does the trained model make predictions? <br/>\n<em>This level of interpretability focus on explaining the overall behaviour of a trained model. This is usually out of reach because complex models like Neural Networks has millions of features and learnable weights. Even a simpler linear model with more than 5 features is difficult for humans to visualise and comprehend.</em></li>\n<li><strong>Global Interpretability (Modular)</strong>: How do parts of the model affect predictions? <br/>\n<em>There is a better chance of understanding part of the models such as a particular weight rather than entire model. However, focus on explaining a single weight without taking account of other weights are often ambiguous. E.g., the number of rooms is positively correlated with the house value given other predictors such as size, age are held constant, which is not the case for real applications.</em></li>\n<li><strong>Local Interpretability (Single Prediction)</strong>: Why did the model make a certain prediction for an instance? <br/>\n<em>Since global interpretability is difficult, we narrow down the explainability to examine what the model predicts with only one instance/example. E.g., to explain the prediction of the model, we look at one particular instance which is a house of 100 squared meters. We then simulate the predicted price changes by increase or decreasing the house size by 10 squared meters gradually. This is often more accurate than global explanations.</em></li>\n<li><strong>Local Interpretability (Group of Predictions)</strong>: Why did the model make specific predictions for a group of instances? <br/>\n<em>Model predictions can be explained with a group of instances/examples. First using global method (#3) on a group of instances (subset of examples), then use local method (#4) on individual instances. Finally, aggregate the entire group for explanation.</em></li>\n</ol>","fields":{"slug":"/posts/what-is-machine-learning-interpretability","tagSlugs":["/tag/interpretability/","/tag/deep-learning/"]},"frontmatter":{"date":"2021-08-08T08:46:37.121Z","description":"Interpretability is the degree to which a human can understand the cause of a decision. -Tom Miller","tags":["Interpretability","Deep Learning"],"title":"What is Machine Learning Interpretability","socialImage":{"publicURL":"/blog/static/606f3769e6b3944838d4279e4001b129/blackbox.png"}}}},"pageContext":{"slug":"/posts/what-is-machine-learning-interpretability"}},"staticQueryHashes":["251939775","401334301","825871152"]}